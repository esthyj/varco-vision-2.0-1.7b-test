import torch
import os
import time
import json
import re
from PIL import Image, ImageDraw, ImageFont
from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration
import textwrap
from glob import glob


#### SSL verification ë¹„í™œì„±í™” ####
os.environ["HF_HUB_DISABLE_SSL_VERIFICATION"] = "1"

import requests
_old_request = requests.sessions.Session.request
def _new_request(self, method, url, **kwargs):
    kwargs["verify"] = False
    return _old_request(self, method, url, **kwargs)
requests.sessions.Session.request = _new_request
###################################


def parse_json_from_output(output: str) -> dict | None:
    """ëª¨ë¸ ì¶œë ¥ì—ì„œ JSON íŒŒì‹±"""
    patterns = [
        r'```json\s*(.*?)\s*```',
        r'```\s*(.*?)\s*```',
        r'(\{[^{}]*\})',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, output, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except json.JSONDecodeError:
                continue
    
    try:
        return json.loads(output)
    except json.JSONDecodeError:
        return None


def visualize_bbox(image_path: str, bbox: list, label: str = "", output_path: str = None):
    """ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ì´ë¯¸ì§€ì— ì‹œê°í™” (ë‹¤ì–‘í•œ ì¢Œí‘œ í˜•ì‹ ì§€ì›)"""
    image = Image.open(image_path).convert("RGB")
    draw = ImageDraw.Draw(image)
    
    img_width, img_height = image.size
    x_min, y_min, x_max, y_max = bbox
    
    print(f"  ğŸ” ì›ë³¸ bbox: {bbox}")
    print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬ê¸°: {img_width}x{img_height}")
    
    # ì¢Œí‘œ ë²”ìœ„ì— ë”°ë¼ ë³€í™˜ ë°©ì‹ ê²°ì •
    max_val = max(bbox)
    
    if max_val <= 1:
        # 0~1 ì •ê·œí™” ì¢Œí‘œ
        x_min = int(x_min * img_width)
        x_max = int(x_max * img_width)
        y_min = int(y_min * img_height)
        y_max = int(y_max * img_height)
    elif max_val <= 1000:
        # 0~1000 ì •ê·œí™” ì¢Œí‘œ
        x_min = int(x_min * img_width / 1000)
        x_max = int(x_max * img_width / 1000)
        y_min = int(y_min * img_height / 1000)
        y_max = int(y_max * img_height / 1000)
    else:
        # ì´ë¯¸ í”½ì…€ ì¢Œí‘œ
        x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)
    
    print(f"  ğŸ“ ë³€í™˜ëœ bbox: [{x_min}, {y_min}, {x_max}, {y_max}]")
    
    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    box_color = (255, 0, 0)
    draw.rectangle([x_min, y_min, x_max, y_max], outline=box_color, width=3)
    
    # ë¼ë²¨ ê·¸ë¦¬ê¸°
    if label:
        try:
            font = ImageFont.truetype("/usr/share/fonts/truetype/nanum/NanumGothic.ttf", 20)
        except:
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 20)
            except:
                font = ImageFont.load_default()
        
        # ë¼ë²¨ì´ ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šë„ë¡ ì¡°ì •
        label_y = max(0, y_min - 25)
        text_bbox = draw.textbbox((x_min, label_y), label, font=font)
        draw.rectangle(text_bbox, fill=box_color)
        draw.text((x_min, label_y), label, fill=(255, 255, 255), font=font)
    
    if output_path is None:
        base, ext = os.path.splitext(image_path)
        output_path = f"{base}_bbox{ext}"
    
    image.save(output_path)
    return image, output_path


def get_image_files(image_dir: str) -> list:
    """ì´ë¯¸ì§€ í´ë”ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°"""
    extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.webp']
    image_files = []
    
    for ext in extensions:
        image_files.extend(glob(os.path.join(image_dir, ext)))
        image_files.extend(glob(os.path.join(image_dir, ext.upper())))
    
    return sorted(image_files)


def process_single_image(model, processor, image_path: str, prompt: str) -> dict:
    """ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬"""
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image", "url": image_path},
                {"type": "text", "text": prompt},
            ],
        },
    ]
    
    # ì „ì²˜ë¦¬
    inputs = processor.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device, torch.float16)
    
    # ì¶”ë¡ 
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    inference_start = time.perf_counter()
    generate_ids = model.generate(**inputs, max_new_tokens=1024)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    inference_time = time.perf_counter() - inference_start
    
    # í›„ì²˜ë¦¬
    generate_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)
    ]
    output = processor.decode(generate_ids_trimmed[0], skip_special_tokens=True)
    num_tokens = len(generate_ids_trimmed[0])
    
    print("LLM ì¶œë ¥:", output)
    return {
        "output": output,
        "inference_time": inference_time,
        "num_tokens": num_tokens,
        "parsed": parse_json_from_output(output)
    }


def main():
    SNAPSHOT_PATH = "/root/.cache/huggingface/hub/models--NCSOFT--VARCO-VISION-2.0-1.7B/snapshots/ed09f37445518b1564d1ef3c6e26fbd7c1b2c818"
    IMAGE_DIR = "../images"
    OUTPUT_DIR = "../images/results"
    
    # ì¶œë ¥ í´ë” ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    image_files = get_image_files(IMAGE_DIR)
    print(f"ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
    for img in image_files:
        print(f"  - {os.path.basename(img)}")
    print("=" * 50)
    
    if not image_files:
        print("âŒ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    # ============ ëª¨ë¸ ë¡œë”© ============
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    load_start = time.perf_counter()
    
    model = LlavaOnevisionForConditionalGeneration.from_pretrained(
        SNAPSHOT_PATH,
        local_files_only=True,
        device_map="auto",
        torch_dtype="auto",
    )
    processor = AutoProcessor.from_pretrained(SNAPSHOT_PATH, local_files_only=True)
    
    load_time = time.perf_counter() - load_start
    print(f"ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.3f}ì´ˆ")
    print("=" * 50)
    
    # ============ í”„ë¡¬í”„íŠ¸ ============
    prompt = textwrap.dedent("""
    ì´ ìë™ì°¨ ê³„ê¸°íŒì—ì„œ ì´ ì£¼í–‰ê±°ë¦¬(ODO)ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

    ê·œì¹™: í™”ë©´ì— ê±°ë¦¬ ê°’ì´ ì—¬ëŸ¬ ê°œ ìˆìœ¼ë©´, **ê°€ì¥ ì•„ë˜ìª½ì— í‘œì‹œëœ ê°’**ì´ ODOì…ë‹ˆë‹¤.

    ì¶œë ¥:
    {
        "odometer_value": "ìˆ«ìê°’",
        "unit": "km ë˜ëŠ” miles",
        "bounding_box": [x_min, y_min, x_max, y_max]
    }
    """).strip()
    
    # ============ ê° ì´ë¯¸ì§€ ì²˜ë¦¬ ============
    results = []
    total_inference_time = 0
    
    for idx, image_path in enumerate(image_files, 1):
        image_name = os.path.basename(image_path)
        print(f"\n[{idx}/{len(image_files)}] ì²˜ë¦¬ ì¤‘: {image_name}")
        print("-" * 40)
        
        try:
            result = process_single_image(model, processor, image_path, prompt)
            total_inference_time += result["inference_time"]
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"  ì¶”ë¡  ì‹œê°„: {result['inference_time']:.3f}ì´ˆ")
            print(f"  í† í° ìˆ˜: {result['num_tokens']}ê°œ")
            
            if result["parsed"]:
                odometer = result["parsed"].get("odometer_value", "N/A")
                unit = result["parsed"].get("unit", "")
                confidence = result["parsed"].get("confidence", "N/A")
                print(f"  âœ… ì£¼í–‰ê±°ë¦¬: {odometer} {unit} (ì‹ ë¢°ë„: {confidence})")
                
                # ë°”ìš´ë”© ë°•ìŠ¤ ì‹œê°í™”
                if "bounding_box" in result["parsed"]:
                    bbox = result["parsed"]["bounding_box"]
                    print(f"  ğŸ” ì›ë³¸ bbox ê°’: {bbox}")
                    label = f"ODO: {odometer} {unit}"
                    output_path = os.path.join(OUTPUT_DIR, f"{os.path.splitext(image_name)[0]}_result.png")
                    visualize_bbox(image_path, bbox, label, output_path)
                    print(f"  ğŸ’¾ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: {output_path}")
            else:
                print(f"  âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨")
                print(f"  ì›ë³¸ ì¶œë ¥: {result['output'][:200]}...")
            
            results.append({
                "image": image_name,
                "success": result["parsed"] is not None,
                "odometer": result["parsed"].get("odometer_value") if result["parsed"] else None,
                "inference_time": result["inference_time"],
                **result
            })
            
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            results.append({
                "image": image_name,
                "success": False,
                "error": str(e)
            })
    
    # ============ ìµœì¢… ìš”ì•½ ============
    print("\n" + "=" * 50)
    print("ğŸ“Š ì „ì²´ ê²°ê³¼ ìš”ì•½")
    print("=" * 50)
    
    successful = sum(1 for r in results if r.get("success"))
    print(f"  ì´ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
    print(f"  ì„±ê³µ: {successful}ê°œ")
    print(f"  ì‹¤íŒ¨: {len(image_files) - successful}ê°œ")
    print(f"  ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.3f}ì´ˆ")
    print(f"  ì´ ì¶”ë¡  ì‹œê°„: {total_inference_time:.3f}ì´ˆ")
    print(f"  í‰ê·  ì¶”ë¡  ì‹œê°„: {total_inference_time / len(image_files):.3f}ì´ˆ/ì´ë¯¸ì§€")
    
    print("\nğŸ“‹ ê°œë³„ ê²°ê³¼:")
    print("-" * 50)
    for r in results:
        status = "âœ…" if r.get("success") else "âŒ"
        odometer = r.get("odometer", "N/A")
        print(f"  {status} {r['image']}: {odometer}")
    
    # ê²°ê³¼ JSON ì €ì¥
    results_file = os.path.join(OUTPUT_DIR, "results.json")
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {results_file}")


if __name__ == "__main__":
    main()