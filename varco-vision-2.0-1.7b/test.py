import torch
import os
import time
from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration


#### SSL verification ë¹„í™œì„±í™” ####
os.environ["HF_HUB_DISABLE_SSL_VERIFICATION"] = "1"

import requests
_old_request = requests.sessions.Session.request
def _new_request(self, method, url, **kwargs):
    kwargs["verify"] = False
    return _old_request(self, method, url, **kwargs)
requests.sessions.Session.request = _new_request
###################################


def main():
    SNAPSHOT_PATH = "/root/.cache/huggingface/hub/models--NCSOFT--VARCO-VISION-2.0-1.7B/snapshots/ed09f37445518b1564d1ef3c6e26fbd7c1b2c818"
    
    # ============ ëª¨ë¸ ë¡œë”© ì‹œê°„ ì¸¡ì • ============
    print("=" * 50)
    print("ëª¨ë¸ ë¡œë”© ì¤‘...")
    load_start = time.perf_counter()
    
    model = LlavaOnevisionForConditionalGeneration.from_pretrained(
        SNAPSHOT_PATH,
        local_files_only=True,
        device_map="auto",
        torch_dtype="auto",
    )
    processor = AutoProcessor.from_pretrained(SNAPSHOT_PATH, local_files_only=True)
    
    load_end = time.perf_counter()
    load_time = load_end - load_start
    print(f"ëª¨ë¸ ë¡œë”© ì‹œê°„: {load_time:.3f}ì´ˆ")
    print("=" * 50)
    
    # ============ ì¶”ë¡  ì¤€ë¹„ ============
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "image", "url": "../images/di1.jpg"},
                {"type": "text", "text": "OCR í›„ HTML í‘œë¡œ ë³€í™˜í•´ì¤˜."},
            ],
        },
    ]
    
    # ============ ì „ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • ============
    print("ì „ì²˜ë¦¬ ì¤‘...")
    preprocess_start = time.perf_counter()
    
    inputs = processor.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        tokenize=True,
        return_dict=True,
        return_tensors="pt"
    ).to(model.device, torch.float16)
    
    preprocess_end = time.perf_counter()
    preprocess_time = preprocess_end - preprocess_start
    print(f"ì „ì²˜ë¦¬ ì‹œê°„: {preprocess_time:.3f}ì´ˆ")
    print("=" * 50)
    
    # ============ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ============
    print("ì¶”ë¡  ì¤‘...")
    
    # GPU ë™ê¸°í™” (ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´)
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    inference_start = time.perf_counter()
    
    generate_ids = model.generate(**inputs, max_new_tokens=1024)
    
    # GPU ë™ê¸°í™” (ì •í™•í•œ ì¸¡ì •ì„ ìœ„í•´)
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    inference_end = time.perf_counter()
    inference_time = inference_end - inference_start
    
    # ============ í›„ì²˜ë¦¬ ============
    generate_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generate_ids)
    ]
    output = processor.decode(generate_ids_trimmed[0], skip_special_tokens=True)
    
    # ============ ê²°ê³¼ ì¶œë ¥ ============
    print("=" * 50)
    print("ìƒì„±ëœ í…ìŠ¤íŠ¸:")
    print(output)
    print("=" * 50)
    
    # ============ ì‹œê°„ ìš”ì•½ ============
    total_time = load_time + preprocess_time + inference_time
    num_tokens = len(generate_ids_trimmed[0])
    tokens_per_sec = num_tokens / inference_time if inference_time > 0 else 0
    
    print("\nğŸ“Š ì‹¤í–‰ ì‹œê°„ ìš”ì•½")
    print("=" * 50)
    print(f"  ëª¨ë¸ ë¡œë”© ì‹œê°„:    {load_time:>8.3f}ì´ˆ")
    print(f"  ì „ì²˜ë¦¬ ì‹œê°„:       {preprocess_time:>8.3f}ì´ˆ")
    print(f"  ì¶”ë¡  ì‹œê°„:         {inference_time:>8.3f}ì´ˆ")
    print("-" * 50)
    print(f"  ì´ ì‹¤í–‰ ì‹œê°„:      {total_time:>8.3f}ì´ˆ")
    print("=" * 50)
    print(f"  ìƒì„±ëœ í† í° ìˆ˜:    {num_tokens}ê°œ")
    print(f"  í† í° ìƒì„± ì†ë„:    {tokens_per_sec:.2f} tokens/sec")
    print("=" * 50)
    
    # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (CUDA ì‚¬ìš© ì‹œ)
    if torch.cuda.is_available():
        print("\nğŸ–¥ï¸  GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
        print("=" * 50)
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / (1024 ** 3)
            reserved = torch.cuda.memory_reserved(i) / (1024 ** 3)
            print(f"  GPU {i}:")
            print(f"    í• ë‹¹ë¨: {allocated:.2f} GB")
            print(f"    ì˜ˆì•½ë¨: {reserved:.2f} GB")
        print("=" * 50)


if __name__ == "__main__":
    main()